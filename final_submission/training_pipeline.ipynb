{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Pycaret\n",
    "#from pycaret.regression import setup\n",
    "import pycaret.nlp as pycnlp\n",
    "import pycaret.classification as pyclass\n",
    "from pycaret.classification import get_config, predict_model, plot_model, pull\n",
    "\n",
    "# MLFlow\n",
    "import mlflow\n",
    "import mlflow.keras\n",
    "from mlflow.tracking import MlflowClient\n",
    "from mlflow.utils.mlflow_tags import MLFLOW_PARENT_RUN_ID, MLFLOW_RUN_NAME\n",
    "\n",
    "# NLP\n",
    "import spacy\n",
    "#spacy_path = MOUNT_PATH + 'libraries/en_core_web_sm-2.3.1'\n",
    "#spacy.load(spacy_path)\n",
    "import category_encoders as ce\n",
    "import pickle\n",
    "\n",
    "# For Coherence Score\n",
    "import gensim\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel, LdaMulticore, CoherenceModel, LsiModel, HdpModel\n",
    "# For Text Vectorization\n",
    "from gensim.models import word2vec\n",
    "\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['font.family'] = ['DejaVu Sans']\n",
    "\n",
    "#pd.set_option('max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Dataset import\n",
    "data_filepath =  \"C:\\\\Users\\\\xtanl\\\\OneDrive\\\\Desktop\\\\data_file_20230730.csv\"\n",
    "data = pd.read_csv(data_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Data Preprocessing\n",
    "data_df = data.copy()\n",
    "data_df = data.drop_duplicates(subset=['content'])\n",
    "### Features addition ###\n",
    "\n",
    "# Fill Nulls in content column\n",
    "data_df['content'] = data_df['content'].fillna('')\n",
    "# Apply spaces behind the hastags to identify hashes\n",
    "data_df['content'] = data_df['content'].apply(lambda x: helper.add_space_hashes(x))\n",
    "# Extract all hashtags\n",
    "data_df['hashtags'] = data_df['content'].apply(lambda x: helper.extract_hashtags(x))\n",
    "# Extract all mentiaons\n",
    "data_df['mentions'] = data_df['content'].apply(lambda x: helper.extract_mentions(x))\n",
    "# Extract all emojis\n",
    "data_df['emojis'] = data_df['content'].apply(lambda x: helper.extract_emojis(x))\n",
    "# Translate Emojis to text\n",
    "data_df['emojis_text'] = data_df['emojis'].apply(lambda x: helper.translate_emojis(x))\n",
    "\n",
    "# Check if there are words to be flagged - breach class\n",
    "data_df['breach_flagwords'] = data_df['content'].apply(lambda x: helper.contains_flagged_words(x))\n",
    "# Check if there are words to be flagged in the hashes - breach class\n",
    "data_df['breach_hashes'] = data_df['hashtags'].apply(lambda x: helper.contains_flagged_hashes(x))\n",
    "\n",
    "# Create label\n",
    "data_df['incompliant'] = np.where((data_df.breach_flagwords == True) | (data_df.breach_hashes == True) , 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = data_df[['Unnamed: 0', 'name', 'content', 'hashtags', 'mentions', 'emojis', 'emojis_text', 'breach_flagwords', 'breach_hashes', 'incompliant']].rename(columns={'Unnamed: 0': 'id'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLFLOW Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mlflow ui\n",
    "exp_id = helper.setup_mlflow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_run = client.create_run(\n",
    "        experiment_id=exp_id,\n",
    "        tags={\n",
    "            MLFLOW_PARENT_RUN_ID : parent_run_id,\n",
    "            MLFLOW_RUN_NAME : f'ml_model_{target_class}',\n",
    "        }\n",
    "    )\n",
    "\n",
    "ml_run_id = ml_run.info.run_uuid\n",
    "client.log_param(ml_run_id, \"run_id\", ml_run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Classification\n",
    "classfication_exp = pyclass.setup(data= train_data.set_index('serial_no').drop(['post_full_text_of_the_post_profile_full_text_of_the_profile', 'cleaned_text', 'category_of_findings'], axis= 1),\n",
    "                                   target = target_class,\n",
    "                                   test_data = validation_data.set_index('serial_no').drop(['post_full_text_of_the_post_profile_full_text_of_the_profile', 'cleaned_text', 'category_of_findings'], axis=1),\n",
    "                                   preprocess=True,\n",
    "                                   silent=True,\n",
    "                                   session_id=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best_model\n",
    "best_model = pyclass.compare_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dataset used for testing models\n",
    "X_train = get_config('X')\n",
    "Y_train = get_config('y')\n",
    "X_test = get_config('X_test')\n",
    "Y_test = get_config('y_test')\n",
    "\n",
    "# Save the dataset\n",
    "todays_date = datetime.today().strftime('%y%m%d')\n",
    "\n",
    "if os.path.exists(filepaths_dict['data_artifact_path'] + f\"{todays_date}_xtrain{parent_run_id[:5]}.csv\"):\n",
    "  print(\"Files have been saved\")\n",
    "else:\n",
    "    X_train.to_csv(filepaths_dict['data_artifact_path'] + f\"{todays_date}_xtrain{parent_run_id[:5]}.csv\")\n",
    "    Y_train.to_csv(filepaths_dict['data_artifact_path'] + f\"{todays_date}_ytrain{parent_run_id[:5]}.csv\")\n",
    "    X_test.to_csv(filepaths_dict['data_artifact_path'] + f\"{todays_date}_xtest{parent_run_id[:5]}.csv\")\n",
    "    Y_test.to_csv(filepaths_dict['data_artifact_path'] + f\"{todays_date}_ytest{parent_run_id[:5]}.csv\")\n",
    "\n",
    "    print(f\"X_Train saved in: {filepaths_dict['data_artifact_path']}\" + f\"{todays_date}_xtrain{parent_run_id[:5]}.csv\")\n",
    "    print(f\"y_Train saved in: {filepaths_dict['data_artifact_path']}\" + f\"{todays_date}_ytrain{parent_run_id[:5]}.csv\")\n",
    "    print(f\"X_test saved in: {filepaths_dict['data_artifact_path']}\" + f\"{todays_date}_xtest{parent_run_id[:5]}.csv\")\n",
    "    print(f\"y_test saved in: {filepaths_dict['data_artifact_path']}\" + f\"{todays_date}_ytest{parent_run_id[:5]}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rule-based Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "xgb_instance = xgb.XGBClassifier()\n",
    "fit_kwargs = { \"early_stopping_rounds\": 5, \"eval_metric\": \"logloss\", \"eval_set\": [(X_test, Y_test)]}\n",
    "xgb_model = pyclass.create_model(xgb_instance, fit_kwargs=fit_kwargs, error_score ='raise')\n",
    "\n",
    "tuned_model = xgb_model\n",
    "print(f'Classifier used: {tuned_model.__class__.__name__}')\n",
    "client.log_param(ml_run_id, \"model_type\", f\"{tuned_model.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pycaret output predictions\n",
    "predictions = predict_model(tuned_model, data=validation_data[validation_data.serial_no.isin(X_test.index)], raw_score=True)\n",
    "\n",
    "# MLflow log result metrics\n",
    "results = pull()\n",
    "results_dict = {k:v[0] for (k,v) in results.to_dict().items() if k != 'Model'}\n",
    "\n",
    "#pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "ml_signature = mlflow.models.infer_signature( model_input = pd.DataFrame(X_train), \n",
    "                                              model_output = pd.DataFrame(rb_test_pred['Label']))\n",
    "\n",
    "mlflow.sklearn.save_model(tuned_model, \n",
    "                        filepaths_dict['ml_artifact_path'],\n",
    "                        signature = ml_signature )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on validation (out of time dataset)\n",
    "predictions['Score_1_round'] = round(predictions['Score_1'], 1)\n",
    "decile_table = get_decile_score(predictions, f'{target_class}', 'Label', 'Score_1_round')\n",
    "\n",
    "decile_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show probability distribution box plot\n",
    "pred_correct = rb_test_pred[rb_test_pred[f'{target_class}'] == rb_test_pred['Label']]\n",
    "prob_dist = pd.DataFrame(pd.Series([round(x*100) for x in pred_correct.Score_1]).value_counts()).reset_index().rename(columns={0: 'count', 'index': 'probability of incompliancy (%)'})\n",
    "prob_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(xgb_model, plot = 'confusion_matrix', plot_kwargs = {'percent' : False})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_run = client.create_run(\n",
    "        experiment_id=exp_id,\n",
    "        tags={\n",
    "            MLFLOW_PARENT_RUN_ID : parent_run_id,\n",
    "            MLFLOW_RUN_NAME : f'lstm_model_{target_class}',\n",
    "        }\n",
    "    )\n",
    "\n",
    "lstm_run_id = lstm_run.info.run_uuid\n",
    "client.log_param(lstm_run_id, \"run_id\", lstm_run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing Data from LSTM Model\n",
    "lstm_fields = ['cleaned_text', 'Category of Findings']\n",
    "\n",
    "X_train = train_data['cleaned_text']\n",
    "X_valid = validation_data['category_of_findings']\n",
    "X_oot = oot_data['category_of_findings']\n",
    "# y_train = np.array(list(map(lambda x: 1 if x==\"No further action required\" else 0, train_data['category_of_findings'])))\n",
    "# y_test = np.array(list(map(lambda x: 1 if x==\"No further action required\" else 0, test_data['category_of_findings'])))\n",
    "\n",
    "y_train = np.array(train_data[f'{target_class}'])\n",
    "y_valid = np.array(validation_data[f'{target_class}'])\n",
    "y_oot = np.array(oot_data[f'{target_class}'])\n",
    "\n",
    "print(\"train_set\", len(X_train), \"valid_set\", len(X_valid), \"oot_set\", len(X_oot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Word Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import one_hot, Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "\n",
    "word_tokenizer = Tokenizer()\n",
    "word_tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# Convert Text to Sequence\n",
    "X_train = word_tokenizer.texts_to_sequences(X_train)\n",
    "X_valid = word_tokenizer.texts_to_sequences(X_valid)\n",
    "X_oot = word_tokenizer.texts_to_sequences(X_oot)\n",
    "\n",
    "# Padding all reviews to fixed length 100\n",
    "maxlen = 100\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "X_valid = pad_sequences(X_valid, padding='post', maxlen=maxlen)\n",
    "X_oot = pad_sequences(X_oot, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log Tokenizer\n",
    "# with open(filepaths_dict['tokenizer_artifact_path'] + 'tokenizer.pkl', 'wb') as outfile:\n",
    "#     pickle.dump(word_tokenizer, outfile)\n",
    "\n",
    "# Save tokenizer\n",
    "with open(filepaths_dict['tokenizer_artifact_path'] + 'tokenizer.pkl', 'wb') as outfile:\n",
    "    pickle.dump(word_tokenizer, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import asarray\n",
    "\n",
    "# Load GloVe word embeddings and create an Embeddings Dictionary\n",
    "embeddings_dictionary = {}\n",
    "glove_file = open('/dbfs/mnt/datahub-apps/ai_critic/libraries/glove/glove.6B.100d.txt', encoding=\"utf8\")\n",
    "\n",
    "for line in glove_file:\n",
    "    records = line.split()\n",
    "    word = records[0]\n",
    "    vector_dimensions = asarray(records[1:], dtype='float32')\n",
    "    embeddings_dictionary [word] = vector_dimensions\n",
    "\n",
    "glove_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding 1 to store dimensions for words for which no pretrained word embeddings exist.+1\n",
    "vocab_length = len(word_tokenizer.word_index) + 1\n",
    "print(\"vocab_length: \", vocab_length)\n",
    "\n",
    "# Create Embedding matrix\n",
    "# Containing 100-dimensional GloVe word embeddings for all words in our corpus.\n",
    "embedding_matrix = np.zeros((vocab_length, 100))\n",
    "for word, index in word_tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_dictionary.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the LSTM model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, GlobalMaxPooling1D, Embedding, Conv1D, LSTM\n",
    "from keras.layers.core import Activation, Dropout, Dense\n",
    "from keras.layers import LSTM ,Bidirectional\n",
    "\n",
    "lstm_model = Sequential()\n",
    "embedding_layer = Embedding(vocab_length, 100, weights=[embedding_matrix], input_length=maxlen , trainable=False)\n",
    "lstm_model.add(embedding_layer)\n",
    "lstm_model.add(Dropout(0.5))\n",
    "lstm_model.add(Bidirectional(LSTM(128)))\n",
    "lstm_model.add(Dense(1, activation='sigmoid')) # Binary\n",
    "\n",
    "# Display Model\n",
    "lstm_model.summary()\n",
    "\n",
    "# Model compiling\n",
    "lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Model Training\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "lstm_model_history = lstm_model.fit(X_train, y_train, batch_size=128, epochs=100, verbose=1, validation_split=0.3, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_lstm(lstm_model, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_acc = evaluate_lstm(lstm_model, X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_lstm_performance(lstm_model_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions in validation dataset for combining with ml models\n",
    "y_pred_valid = lstm_model.predict(X_train)\n",
    "y_pred_test = lstm_model.predict(X_valid)\n",
    "\n",
    "lstm_test_pred = pd.DataFrame(y_pred_test, columns=['lstm_pred_score'])\n",
    "lstm_test_pred['lstm_pred'] = np.where(lstm_test_pred['lstm_pred_score'] < 0.50, 0, 1)\n",
    "lstm_test_pred = lstm_test_pred.merge(pd.DataFrame(validation_data['serial_no']).reset_index(drop=True), how='left', left_index=True, right_index=True)\n",
    "\n",
    "lstm_valid_pred = pd.DataFrame(y_pred_valid, columns=['lstm_pred_score'])\n",
    "lstm_valid_pred['lstm_pred'] = np.where(lstm_valid_pred['lstm_pred_score'] < 0.50, 0, 1)\n",
    "lstm_valid_pred = lstm_valid_pred.merge(pd.DataFrame(validation_data['serial_no']).reset_index(drop=True), how='left', left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions in OOT dataset for testing\n",
    "y_pred_oot = lstm_model.predict(X_oot)\n",
    "\n",
    "lstm_oot_pred = pd.DataFrame(y_pred_oot, columns=['lstm_pred_score'])\n",
    "lstm_oot_pred['lstm_pred'] = np.where(lstm_oot_pred['lstm_pred_score'] < 0.50, 0, 1)\n",
    "lstm_oot_pred = lstm_oot_pred.merge(pd.DataFrame(oot_data['serial_no']).reset_index(drop=True), how='left', left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "dl_signature = mlflow.models.infer_signature( model_input = X_oot,\n",
    "                                              model_output = y_pred_valid )\n",
    "\n",
    "mlflow.keras.save_model(lstm_model,\n",
    "                        filepaths_dict['dl_artifact_path'],\n",
    "                        signature = dl_signature )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined prediction labels - validation data\n",
    "rule_based_prediction = rb_valid_pred[[f'{target_class}', 'Label', 'Score_1']].rename({'Label': 'rb_pred', 'Score_1':'rb_pred_score'}, axis=1)\n",
    "combined_pred = lstm_valid_pred.merge(rule_based_prediction, how='left', left_index=True, right_index=True)\n",
    "\n",
    "# Combined prediction labels - oot data\n",
    "rule_based_test = rb_test_pred[[f'{target_class}', 'Label', 'Score_1']].rename({'Label': 'rb_pred', 'Score_1':'rb_pred_score'}, axis=1)\n",
    "combined_test = lstm_test_pred.merge(rule_based_test, how='left', left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine split between models\n",
    "correct = []\n",
    "\n",
    "for dec_increment in range(40, 101):\n",
    "  lstm_wt = dec_increment/ 100\n",
    "  combined_pred['combined_score'] = (lstm_wt*combined_pred['lstm_pred_score'] + (1-lstm_wt)*combined_pred['rb_pred_score'])\n",
    "  combined_pred['combined_pred'] = np.where(combined_pred['combined_score'] < 0.50, 0, 1)\n",
    "  right_prop = (combined_pred['combined_pred'] == combined_pred[f'{target_class}']).value_counts().iloc[0] / len(combined_pred)\n",
    "  correct.append((lstm_wt,right_prop))\n",
    "\n",
    "correct = pd.DataFrame(correct).rename({0: 'weight', 1:'accuracy'}, axis=1)\n",
    "plt.plot(correct['weight'], correct['accuracy'])\n",
    "plt.ylabel(\"Combined Accuracy (%)\")\n",
    "plt.xlabel(\"Proportion of LSTM prediction scores used\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the weights that produce the best score\n",
    "lstm_wt = correct[correct.accuracy == correct.accuracy.max()]['weight'].min()\n",
    "\n",
    "# Validate on test data (2023)\n",
    "combined_test['combined_score'] = (lstm_wt*combined_test['lstm_pred_score'] + (1-lstm_wt)*combined_test['rb_pred_score'])\n",
    "combined_test['combined_pred'] = np.where(combined_test['combined_score'] < 0.5, 0, 1)\n",
    "\n",
    "final_acc = (combined_test['combined_pred'] == combined_test[f'{target_class}']).value_counts().iloc[0] / len(combined_test)\n",
    "# Rearrange columns\n",
    "combined_test = combined_test[['serial_no', f'{target_class}', 'combined_score', 'combined_pred', 'lstm_pred_score', 'lstm_pred',\n",
    "                               'rb_pred_score', 'rb_pred']]\n",
    "combined_test.to_csv( filepaths_dict['exai_artifact_path'] + f\"prediction_{todays_date}.csv\")\n",
    "\n",
    "print(f\"Combined Accuracy for {lstm_wt} weight:\", round(final_acc, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Log artifacts and metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log all artifacts\n",
    "with mlflow.start_run(run_id=lstm_run_id):\n",
    "    mlflow.log_metrics(metrics={\"Accuracy\": lstm_acc})\n",
    "\n",
    "    mlflow.log_artifacts(filepaths_dict['exai_artifact_path'], \"Results\")\n",
    "    mlflow.log_artifacts(filepaths_dict['tokenizer_artifact_path'], \"Tokenizer\")\n",
    "    mlflow.log_artifacts(filepaths_dict['dl_artifact_path'], \"model\")\n",
    "\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log final metrics\n",
    "with mlflow.start_run(run_id=parent_run_id):\n",
    "    mlflow.log_param(\"model_wt\", lstm_wt)\n",
    "    mlflow.log_metrics(metrics={\"Accuracy\": round(final_acc, 3)})\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"run_id: {parent_run.info.run_id}; status: {parent_run.info.status}\")\n",
    "print(f\"run_id: {ml_run.info.run_id}; status: {ml_run.info.status}\")\n",
    "print(f\"run_id: {lstm_run.info.run_id}; status: {lstm_run.info.status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Register Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if all basic metrics are above/Below the threshold\n",
    "if ((final_acc > 0.6)):\n",
    "  is_model_good = 'yes'\n",
    "else:\n",
    "  is_model_good = 'no'\n",
    "\n",
    "print(\"is_model_good: \", is_model_good, \", final_accuracy is\", final_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check existing model and compare, Based on outcome register this new model\n",
    "if is_model_good == 'yes':\n",
    "    mlflow_existing_model_compare_and_registry(ml_run, f'XGBoost_{target_class}', ['accuracy'], [0.01], \"Classification\")\n",
    "    mlflow_existing_model_compare_and_registry(lstm_run, f'LSTM_{target_class}', ['accuracy'], [0.01], \"Classification\")\n",
    "else:\n",
    "    None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_critic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
