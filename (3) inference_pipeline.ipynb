{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# import pycaret\n",
    "# from pycaret.classification import *\n",
    "\n",
    "# MLFlow\n",
    "import mlflow\n",
    "import mlflow.keras\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "# LSTM\n",
    "import keras\n",
    "# from keras.preprocessing.text import one_hot, Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "\n",
    "import src.helpers_preprocess as pp\n",
    "import src.helpers_mlflow as mlf\n",
    "import src.config as config\n",
    "\n",
    "# import importlib\n",
    "# importlib.reload(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_class = 'incompliant'\n",
    "SAVE_PREDICTIONS = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filepath already exists\n",
      "Filepath already exists\n",
      "Filepath already exists\n",
      "Filepath already exists\n",
      "Filepath already exists\n"
     ]
    }
   ],
   "source": [
    "# Check if filepaths exists and create filepaths if do not exist\n",
    "# Explainations for filepaths can be found in config.py\n",
    "config.create_path(config.main_directory)\n",
    "config.create_path(config.output_path)\n",
    "config.create_path(config.data_artifact_path)\n",
    "config.create_path(config.tokenizer_artifact_path)\n",
    "config.create_path(config.inference_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import features dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset from C:\\Users\\xtanl\\OneDrive - Singapore Management University\\Capstone\\raw_data\\full_features_231106.csv\n"
     ]
    }
   ],
   "source": [
    "# Get latest feature set\n",
    "all_feature_files = [os.path.join(config.raw_data_path, x) for x in os.listdir(config.raw_data_path) if x.startswith(\"full_features\") and x.endswith(\".csv\")]\n",
    "curr_features_filepath = max(all_feature_files, key = os.path.getctime)\n",
    "data_df = pd.read_csv(curr_features_filepath, index_col= None)\n",
    "print(f\"Dataset from {curr_features_filepath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load MLFLOW Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup the MYSQL database for tracking MLFLOW Model Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database created at MYSQL: mlflow_tracking_database on root:isss625@127.0.0.1/3306\n"
     ]
    }
   ],
   "source": [
    "# Create database if it does not exists\n",
    "# Database required for MLFlow model registry as only certain APIs are supported by Mlflow\n",
    "mlf.create_database_storage(config.dbServerName, config.dbPort, config.dbName, config.dbUser, config.dbPassword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if database exists\n",
    "# mlf.show_databases(dbServerName, dbUser, dbPassword)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup MLFLOW to retrieve experiments and model registry (in MYSQL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mlflow command run configured in src/config.py\n",
    "mlflow_conn = mlf.create_mlflow_cmd(config.storage_filepath, config.dbName, config.dbUser, config.dbPassword, config.dbServerName, config.dbPort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running command line with timeout of 10 seconds \n",
      "mlflow ui                     --backend-store-uri file:/./aicritic_mlflow                     --registry-store-uri mysql+pymysql://root:isss625@localhost:3306/mlflow_tracking_database                     --host 127.0.0.1 --port 5000                     --serve-artifacts\n"
     ]
    }
   ],
   "source": [
    "# Run mlflow command line\n",
    "mlf.run_cmd(mlflow_conn, config.timeout)\n",
    "print(f\"Running command line with timeout of {config.timeout} seconds \\n{mlflow_conn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLFLOW UI is at: http://127.0.0.1:5000/\n",
      "RESOURCE_ALREADY_EXISTS: Experiment 'ai_critic' already exists.\n",
      "exp_id 0\n"
     ]
    }
   ],
   "source": [
    "exp_id, client = mlf.setup_mlflow(config.exp_name, config.storage_filepath)\n",
    "print(\"exp_id\", exp_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Rule Based Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get production ML model from Model Registry\n",
    "ml_model_uri = f\"models:/XGBoost_{target_class}/Production\"\n",
    "ml_model = mlflow.sklearn.load_model(model_uri=ml_model_uri)\n",
    "input_cols = ml_model.feature_names_in_\n",
    "\n",
    "categorical_cols = [#'contains_montary', \n",
    "                    'breach_flagwords', 'breach_hashes', 'has_nonpru_email', 'has_hyperlinks', 'has_disclaimer']\n",
    "\n",
    "# OH Encode\n",
    "data_encoded, enc_cols = pp.get_onehot(data_df, feature_list = categorical_cols)\n",
    "\n",
    "# Align Columns\n",
    "pred_data = pp.column_alignment(new_dataset=data_encoded, loaded_featset=input_cols, cat_feats=['breach_flagwords', 'breach_hashes', 'has_nonpru_email',\n",
    "                                                                                                'has_hyperlinks', 'has_approvals', 'has_disclaimer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rule-Based Predictions\n",
    "rb_predictions = ml_model.predict(pred_data)\n",
    "rb_predictions = pd.DataFrame(data_encoded['id']).merge(pd.DataFrame(rb_predictions, columns= ['rb_pred']), left_index= True, right_index=True)\n",
    "\n",
    "predict_scores = pd.DataFrame(pd.DataFrame(ml_model.predict_proba(pred_data))[1]).rename({1: 'rb_pred_score'}, axis=1)\n",
    "predict_scores['rb_pred_score'] = predict_scores['rb_pred_score'].astype('float')\n",
    "rb_predictions = rb_predictions.merge(predict_scores, left_index= True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predictions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\xtanl\\OneDrive - Singapore Management University\\Capstone\\final_submission\\inference_pipeline.ipynb Cell 20\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/xtanl/OneDrive%20-%20Singapore%20Management%20University/Capstone/final_submission/inference_pipeline.ipynb#X46sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Show probability distribution box plot\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/xtanl/OneDrive%20-%20Singapore%20Management%20University/Capstone/final_submission/inference_pipeline.ipynb#X46sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m pred_correct \u001b[39m=\u001b[39m predictions[predictions[\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mtarget_class\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m predictions[\u001b[39m'\u001b[39m\u001b[39mprediction_label\u001b[39m\u001b[39m'\u001b[39m]]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/xtanl/OneDrive%20-%20Singapore%20Management%20University/Capstone/final_submission/inference_pipeline.ipynb#X46sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m prob_dist \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(pd\u001b[39m.\u001b[39mSeries([\u001b[39mround\u001b[39m(x\u001b[39m*\u001b[39m\u001b[39m100\u001b[39m) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m pred_correct\u001b[39m.\u001b[39mScore_1_round])\u001b[39m.\u001b[39mvalue_counts())\u001b[39m.\u001b[39mreset_index()\u001b[39m.\u001b[39mrename(columns\u001b[39m=\u001b[39m{\u001b[39m0\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mcount\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mprobability of incompliancy (\u001b[39m\u001b[39m%\u001b[39m\u001b[39m)\u001b[39m\u001b[39m'\u001b[39m})\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/xtanl/OneDrive%20-%20Singapore%20Management%20University/Capstone/final_submission/inference_pipeline.ipynb#X46sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m prob_dist\n",
      "\u001b[1;31mNameError\u001b[0m: name 'predictions' is not defined"
     ]
    }
   ],
   "source": [
    "# Show probability distribution box plot\n",
    "pred_correct = predictions[predictions[f'{target_class}'] == predictions['prediction_label']]\n",
    "prob_dist = pd.DataFrame(pd.Series([round(x*100) for x in pred_correct.Score_1_round]).value_counts()).reset_index().rename(columns={0: 'count', 'index': 'probability of incompliancy (%)'})\n",
    "prob_dist\n",
    "\n",
    "if SAVE_PROB_DIST:\n",
    "    # Save probabliity distribution table\n",
    "    prob_dist.to_csv(os.path.join(config.exai_artifact_path, f\"prob_dist_{config.today}.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model_uri = f\"models:/LSTM_{target_class}/Production\"\n",
    "nn_model = mlflow.keras.load_model(model_uri=nn_model_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_runid:  9134773ea386403d9403dc7fa8ab6690\n"
     ]
    }
   ],
   "source": [
    "# Get LSTM production model run_id\n",
    "for mv in client.get_latest_versions(name=f\"LSTM_{target_class}\"):\n",
    "    if dict(mv)['current_stage'] == 'Production':\n",
    "      nn_model_runid = dict(mv)['run_id']\n",
    "\n",
    "print(\"model_runid: \", nn_model_runid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Word Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer\n",
    "tokenizer_uri = f\"runs:/{nn_model_runid}/Tokenizer/tokenizer.pkl\"\n",
    "mlflow.artifacts.download_artifacts(artifact_uri=tokenizer_uri, dst_path=config.tokenizer_artifact_path)\n",
    "with open(os.path.join(config.tokenizer_artifact_path, \"tokenizer.pkl\"), 'rb') as outfile:\n",
    "    word_tokenizer = pickle.load(outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Text to Sequence\n",
    "data_tokenized = word_tokenizer.texts_to_sequences(data_df)\n",
    "\n",
    "# Padding all reviews to fixed length 100\n",
    "maxlen = 100\n",
    "data_tokenized = pad_sequences(data_tokenized, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import asarray\n",
    "\n",
    "# Load GloVe word embeddings and create an Embeddings Dictionary\n",
    "embeddings_dictionary = {}\n",
    "glove_file = open(config.glove_file, encoding=\"utf8\")\n",
    "\n",
    "for line in glove_file:\n",
    "    records = line.split()\n",
    "    word = records[0]\n",
    "    vector_dimensions = asarray(records[1:], dtype='float32')\n",
    "    embeddings_dictionary [word] = vector_dimensions\n",
    "\n",
    "glove_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_length:  16355\n"
     ]
    }
   ],
   "source": [
    "# Adding 1 to store dimensions for words for which no pretrained word embeddings exist.\n",
    "vocab_length = len(word_tokenizer.word_index) + 1\n",
    "print(\"vocab_length: \", vocab_length)\n",
    "\n",
    "# Create Embedding matrix\n",
    "# Containing 100-dimensional GloVe word embeddings for all words in our corpus.\n",
    "embedding_matrix = np.zeros((vocab_length, 100))\n",
    "for word, index in word_tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_dictionary.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load LSTM MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 710ms/step\n"
     ]
    }
   ],
   "source": [
    "# Get production model from Model Registry\n",
    "lstm_model_uri = f\"models:/LSTM_{target_class}/Production\"\n",
    "lstm_model = mlflow.keras.load_model(model_uri=nn_model_uri)\n",
    "lstm_pred = lstm_model.predict(data_tokenized)\n",
    "\n",
    "lstm_prediction = pd.DataFrame(lstm_pred, columns=['lstm_pred_score'])\n",
    "lstm_prediction['lstm_pred_score'] = lstm_prediction['lstm_pred_score'].astype('float')\n",
    "lstm_prediction['lstm_pred'] = np.where(lstm_prediction['lstm_pred_score'] < 0.50, 0, 1)\n",
    "lstm_prediction = lstm_prediction.merge(pd.DataFrame(data_df['id']).reset_index(drop=True), how='left', left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined prediction labels\n",
    "rule_based_prediction = rb_predictions[['rb_pred', 'rb_pred_score']]\n",
    "combined_pred = lstm_prediction.merge(rule_based_prediction, how='left', left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the weights that produce the best score\n",
    "# parent_run = dict(mlflow.get_parent_run(nn_model_runid))\n",
    "parent_runid = dict(dict(mlflow.get_run(nn_model_runid))['data'])['tags']['mlflow.parentRunId']\n",
    "parent_run = dict(mlflow.get_run(parent_runid))\n",
    "lstm_wt = float(dict(parent_run['data'])['params']['model_wt'])\n",
    "combined_pred['combined_score'] = (lstm_wt*combined_pred['lstm_pred_score'] + (1-lstm_wt)* combined_pred['rb_pred_score'])\n",
    "combined_pred['combined_pred'] = np.where(combined_pred['combined_score'] < 0.5, 0, 1)\n",
    "# Rearrange columns\n",
    "combined_pred = combined_pred[['id', 'combined_score', 'combined_pred', 'lstm_pred_score', 'lstm_pred',\n",
    "                               'rb_pred_score', 'rb_pred']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions\n",
    "if SAVE_PREDICTIONS:\n",
    "    combined_pred.to_csv(os.path.join(config.inference_output, f\"prediction_{config.today}.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>combined_score</th>\n",
       "      <th>combined_pred</th>\n",
       "      <th>lstm_pred_score</th>\n",
       "      <th>lstm_pred</th>\n",
       "      <th>rb_pred_score</th>\n",
       "      <th>rb_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>pfbid0cPYXbGHKhc8dDR7aersC4nyxbXCTLP5vFpsVJe1K...</td>\n",
       "      <td>0.602089</td>\n",
       "      <td>1</td>\n",
       "      <td>0.023425</td>\n",
       "      <td>0</td>\n",
       "      <td>0.987866</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  id  combined_score  \\\n",
       "7  pfbid0cPYXbGHKhc8dDR7aersC4nyxbXCTLP5vFpsVJe1K...        0.602089   \n",
       "\n",
       "   combined_pred  lstm_pred_score  lstm_pred  rb_pred_score  rb_pred  \n",
       "7              1         0.023425          0       0.987866        1  "
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview output\n",
    "# combined_pred[combined_pred['combined_pred'] == 1].head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of Script"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
