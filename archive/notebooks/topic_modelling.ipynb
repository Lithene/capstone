{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "086bd700",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "import unicodedata\n",
    "import emoji\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import warnings\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47a51f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download datafile from sharepoint\n",
    "data_file_path = \"C:\\\\Users\\\\xtanl\\\\OneDrive - Singapore Management University\\\\Capstone\\\\inputs\\\\preprocessed_230604.xlsx\"\n",
    "data = pd.read_excel(data_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7092524d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>content</th>\n",
       "      <th>breach</th>\n",
       "      <th>non_compliant</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>mentions</th>\n",
       "      <th>emojis</th>\n",
       "      <th>emojis_text</th>\n",
       "      <th>breach_flagwords</th>\n",
       "      <th>breach_hashes</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nicholas Goh Organisation</td>\n",
       "      <td>Follow us at NGO’s BKK FastTrack 2022! In celebrating with the qualifiers, we had a lot of fun. Our team really bonded while traveling, eating, and shopping together! A true companion is one that travels together and stays together! It was 100% more enjoyable to spend time traveling together than going alone! Once again, congratulations to all the qualifiers!</td>\n",
       "      <td>No further action required</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>['Follow', 'us', 'NGO’s', 'BKK', 'FastTrack', 'In', 'celebrating', 'qualifiers', 'lot', 'fun', 'Our', 'team', 'really', 'bonded', 'traveling', 'eating', 'shopping', 'together', 'A', 'true', 'companion', 'one', 'travels', 'together', 'stays', 'together', 'It', 'enjoyable', 'spend', 'time', 'traveling', 'together', 'going', 'alone', 'Once', 'congratulations', 'qualifiers']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        name  \\\n",
       "0  Nicholas Goh Organisation   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                     content  \\\n",
       "0  Follow us at NGO’s BKK FastTrack 2022! In celebrating with the qualifiers, we had a lot of fun. Our team really bonded while traveling, eating, and shopping together! A true companion is one that travels together and stays together! It was 100% more enjoyable to spend time traveling together than going alone! Once again, congratulations to all the qualifiers!   \n",
       "\n",
       "                       breach  non_compliant hashtags mentions emojis  \\\n",
       "0  No further action required              0       []       []    NaN   \n",
       "\n",
       "  emojis_text  breach_flagwords  breach_hashes  \\\n",
       "0         NaN             False          False   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                            cleaned_text  \n",
       "0  ['Follow', 'us', 'NGO’s', 'BKK', 'FastTrack', 'In', 'celebrating', 'qualifiers', 'lot', 'fun', 'Our', 'team', 'really', 'bonded', 'traveling', 'eating', 'shopping', 'together', 'A', 'true', 'companion', 'one', 'travels', 'together', 'stays', 'together', 'It', 'enjoyable', 'spend', 'time', 'traveling', 'together', 'going', 'alone', 'Once', 'congratulations', 'qualifiers']  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8433fbe",
   "metadata": {},
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d60ed58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.DataFrame(data_df.iloc[37]).transpose() #23 - chinese, 37 emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80b30d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1975e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def text_cleaning(text):\n",
    "    # remove tags\n",
    "    text = re.sub(\"@\\S+\", \"\", str(text))\n",
    "    # remove websites\n",
    "    text = re.sub(\"https*\\S+\", \"\", str(text))\n",
    "    # remove hashtags\n",
    "    text = re.sub(\"#\\S+\", \"\", str(text))\n",
    "    # remove apostrophes eg. abc's\n",
    "    text = re.sub(\"\\'\\w+\", \"\", str(text))\n",
    "    # remove punctuation\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), \"\", str(text))\n",
    "    # remove numbers\n",
    "    text = re.sub(r'\\w*\\d+\\w*', \"\", str(text))\n",
    "    # lowercase -- to remove stopwords\n",
    "    text = text.lower()\n",
    "    # remove stopwords\n",
    "    default_stopwords = stopwords.words('english')\n",
    "    text = \" \".join(word for word in text.split() if word not in default_stopwords)\n",
    "    \n",
    "    # remove spaces more than \" \"\n",
    "    #text = re.sub('\\s{2,}', \" \", text)\n",
    "\n",
    "    return [word for word in text.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61e23de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_stopwords = stopwords.words('english')\n",
    "#pd.Series(default_stopwords)[pd.Series(default_stopwords).str.contains('our')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5afb08c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_chinese(cleaned):\n",
    "    \"\"\"\n",
    "    cleaned: data_df['cleaned_text']\n",
    "    \"\"\"\n",
    "    \n",
    "    no_chinese = []\n",
    "    CHINESE_REGEX = r'[\\u4e00-\\u9fff]+' #r'[^\\x00-\\x7F]+'\n",
    "\n",
    "    for post in cleaned:\n",
    "        new_post = []\n",
    "        for term in post:\n",
    "            term.replace(CHINESE_REGEX, '')\n",
    "            if re.findall(CHINESE_REGEX, term) == []: # find chinese chars\n",
    "                new_post.append(term)\n",
    "        no_chinese.append(new_post)\n",
    "        \n",
    "    return no_chinese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e1cdf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_emojis(cleaned):\n",
    "    emoj = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  # dingbats\n",
    "        u\"\\u3030\"\n",
    "                      \"]+\", re.UNICODE)\n",
    "    \n",
    "    no_emoji = []\n",
    "    \n",
    "    for post in cleaned:\n",
    "        new_post = []\n",
    "        for term in post:\n",
    "            term = re.sub(emoj, '', term)\n",
    "            new_post.append(term)\n",
    "            \n",
    "        no_emoji.append(new_post)\n",
    "        \n",
    "    return no_emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b3afc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "def normalized_corpus(corpus):\n",
    "    \n",
    "    lemma = WordNetLemmatizer()\n",
    "    # Normalize a list of words     \n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in corpus)\n",
    "    # remove stopwords\n",
    "    default_stopwords = stopwords.words('english')\n",
    "    text = \" \".join(word for word in normalized.split() if word not in default_stopwords)\n",
    "    \n",
    "    return [word for word in text.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9d8b568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-Clean text\n",
    "data_df['cleaned_text'] = data_df.content.apply(lambda x: text_cleaning(x))\n",
    "# Remove chinese\n",
    "data_df['remove_chinese'] = remove_chinese(data_df['cleaned_text'])\n",
    "# Remove emojis\n",
    "data_df['remove_emojis'] = remove_emojis(data_df['remove_chinese'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef39321a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize text\n",
    "data_df['normalised_text'] = data_df['remove_emojis'].apply(lambda x: normalized_corpus(x))\n",
    "# Convert to a list of list of lemmatized words\n",
    "doc_clean = [text for text in data_df['normalised_text']] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e53de12",
   "metadata": {},
   "source": [
    "### Document Term Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac029995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Gensim\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "# Creating the term dictionary of our courpus, where every unique term is assigned an index. \n",
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "\n",
    "# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b8a9e9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the object for LDA model using gensim library\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "# Running and Trainign LDA model on the document term matrix.\n",
    "NUM_TOPICS = 3\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=NUM_TOPICS, id2word = dictionary, passes=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9056cd0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.020*\"insurance\" + 0.012*\"financial\" + 0.010*\"plan\" + 0.010*\"life\" + 0.008*\"love\" + 0.008*\"income\" + 0.008*\"family\" + 0.007*\"coverage\" + 0.007*\"money\" + 0.007*\"way\"') \n",
      "\n",
      "(1, '0.009*\"thing\" + 0.008*\"happy\" + 0.007*\"yes\" + 0.007*\"great\" + 0.007*\"gst\" + 0.007*\"million\" + 0.007*\"dollar\" + 0.005*\"table\" + 0.005*\"achievement\" + 0.005*\"never\"') \n",
      "\n",
      "(2, '0.012*\"thank\" + 0.012*\"year\" + 0.011*\"birthday\" + 0.011*\"day\" + 0.009*\"special\" + 0.009*\"wish\" + 0.008*\"ngo\" + 0.008*\"u\" + 0.007*\"made\" + 0.007*\"one\"') \n",
      "\n"
     ]
    }
   ],
   "source": [
    "NUM_WORDS = 10\n",
    "\n",
    "for topic in range(len(ldamodel.print_topics(num_words=NUM_WORDS))):\n",
    "    print(ldamodel.print_topics(num_words=NUM_WORDS)[topic], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cfaed848",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(ldamodel.print_topics(num_words=10)[0]) #num_topics=10, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "141ed432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature vector\n",
    "train_vecs = []\n",
    "for i in range(len(data_df)):\n",
    "    top_topics = ldamodel.get_document_topics(train_corpus[i], minimum_probability=0.0)\n",
    "    topic_vec = [top_topics[i][1] for i in range(NUM_TOPICS)]\n",
    "    topic_vec.extend([len(data_df.iloc[i].cleaned_text)]) # length review\n",
    "    train_vecs.append(topic_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a57392",
   "metadata": {},
   "source": [
    "### Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1e5a2214",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigrams(words, bi_min=15, tri_min=10):\n",
    "    bigram = gensim.models.Phrases(words, min_count = bi_min)\n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    return bigram_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f06420fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corpus(content):\n",
    "    words = [text for text in content] \n",
    "    bigram_mod = bigrams(words)\n",
    "    bigram = [bigram_mod[post] for post in words]\n",
    "    id2word = gensim.corpora.Dictionary(bigram)\n",
    "    id2word.filter_extremes(no_below=10, no_above=0.35)\n",
    "    id2word.compactify()\n",
    "    corpus = [id2word.doc2bow(text) for text in bigram]\n",
    "    return corpus, id2word, bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8164291b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus, train_id2word, bigram_train = get_corpus(data_df['normalised_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c7df64",
   "metadata": {},
   "source": [
    "### Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fc44a734",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn import linear_model\n",
    "\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4dbcda3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Val f1: 0.247 +- 0.106\n",
      "Logisitic Regression SGD Val f1: 0.527 +- 0.203\n",
      "SVM Huber Val f1: 0.227 +- 0.187\n"
     ]
    }
   ],
   "source": [
    "X = np.array(train_vecs)\n",
    "y = np.array(data_df.non_compliant)\n",
    "\n",
    "kf = KFold(5, shuffle=True, random_state=42)\n",
    "cv_lr_f1, cv_lrsgd_f1, cv_svcsgd_f1,  = [], [], []\n",
    "\n",
    "for train_ind, val_ind in kf.split(X, y):\n",
    "    # Assign CV IDX\n",
    "    X_train, y_train = X[train_ind], y[train_ind]\n",
    "    X_val, y_val = X[val_ind], y[val_ind]\n",
    "    \n",
    "    # Scale Data\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scale = scaler.fit_transform(X_train)\n",
    "    X_val_scale = scaler.transform(X_val)\n",
    "\n",
    "    # Logisitic Regression\n",
    "    lr = LogisticRegression(\n",
    "        class_weight= 'balanced',\n",
    "        solver='newton-cg',\n",
    "        fit_intercept=True\n",
    "    ).fit(X_train_scale, y_train)\n",
    "\n",
    "    y_pred = lr.predict(X_val_scale)\n",
    "    cv_lr_f1.append(f1_score(y_val, y_pred, average='binary'))\n",
    "    \n",
    "    # Logistic Regression SGD\n",
    "    sgd = linear_model.SGDClassifier(\n",
    "        max_iter=1000,\n",
    "        tol=1e-3,\n",
    "        loss='log_loss',\n",
    "        class_weight='balanced'\n",
    "    ).fit(X_train_scale, y_train)\n",
    "    \n",
    "    y_pred = sgd.predict(X_val_scale)\n",
    "    cv_lrsgd_f1.append(f1_score(y_val, y_pred, average='binary'))\n",
    "    \n",
    "    # SGD Modified Huber\n",
    "    sgd_huber = linear_model.SGDClassifier(\n",
    "        max_iter=1000,\n",
    "        tol=1e-3,\n",
    "        alpha=20,\n",
    "        loss='modified_huber',\n",
    "        class_weight='balanced'\n",
    "    ).fit(X_train_scale, y_train)\n",
    "    \n",
    "    y_pred = sgd_huber.predict(X_val_scale)\n",
    "    cv_svcsgd_f1.append(f1_score(y_val, y_pred, average='binary'))\n",
    "\n",
    "print(f'Logistic Regression Val f1: {np.mean(cv_lr_f1):.3f} +- {np.std(cv_lr_f1):.3f}')\n",
    "print(f'Logisitic Regression SGD Val f1: {np.mean(cv_lrsgd_f1):.3f} +- {np.std(cv_lrsgd_f1):.3f}')\n",
    "print(f'SVM Huber Val f1: {np.mean(cv_svcsgd_f1):.3f} +- {np.std(cv_svcsgd_f1):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebac10a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
