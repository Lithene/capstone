{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6126b25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "import unicodedata\n",
    "import emoji\n",
    "\n",
    "import string\n",
    "from itertools import permutations, combinations\n",
    "from nltk import WordPunctTokenizer, BigramAssocMeasures, TrigramAssocMeasures, QuadgramAssocMeasures\n",
    "from nltk.collocations import BigramCollocationFinder, TrigramCollocationFinder, QuadgramCollocationFinder\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import helpers as helper\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1300460b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download datafile from sharepoint\n",
    "data_file_path = \"C:\\\\Users\\\\xtanl\\\\OneDrive - Singapore Management University\\\\Capstone\\\\inputs\\\\preprocessed_230604.xlsx\"\n",
    "data = pd.read_excel(data_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e717131",
   "metadata": {},
   "source": [
    "## Rule Based Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437f48d5",
   "metadata": {},
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766cc594",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf51d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-Clean text\n",
    "data_df['cleaned_text'] = data_df.content.apply(lambda x: helper.text_cleaning(x))\n",
    "# Remove emojis\n",
    "data_df['remove_emojis'] = helper.remove_emojis(data_df['cleaned_text'])\n",
    "# Remove chinese\n",
    "data_df['remove_chinese'] = helper.remove_chinese(data_df['remove_emojis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abddd6fe",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "data_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d044a89",
   "metadata": {},
   "source": [
    "### Get list of terms and synthesize new words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd09fac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def permutate(single_string):\n",
    "    \"\"\"\n",
    "    INPUT\n",
    "    single_string: a string of different terms\n",
    "    OUTPUT\n",
    "    returns a list of different permutations of the original string of terms\n",
    "    \"\"\"\n",
    "    # split string by space or brackets\n",
    "    string_list = re.split(r\"[()| ]+\", single_string)\n",
    "    word_perm = [\" \".join(items) for items in permutations(string_list, r=len(string_list))]\n",
    "\n",
    "    return set(word_perm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b138268a",
   "metadata": {},
   "outputs": [],
   "source": [
    "permutate(\"PruShield Premium Plus (Foreigner)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68c2c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_combinations(single_string):\n",
    "    \"\"\"\n",
    "    INPUT\n",
    "    single_string: a string of different terms\n",
    "    OUTPUT\n",
    "    returns a list of different combinations of the original string of terms\n",
    "    \"\"\"\n",
    "    # remove punctuation\n",
    "    single_string = re.sub('[%s]' % re.escape(string.punctuation), \"\", single_string)\n",
    "    # split string by space or brackets\n",
    "    string_list = re.split(r\"[()| ]+\", single_string)\n",
    "    word_combi = [\" \".join(items) for items in combinations(string_list, r=len(string_list)-1)]\n",
    "\n",
    "    return set(word_combi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64ae5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_combinations(\"PruShield Premium Plus (Foreigner)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f47cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthesize_words(word_list):\n",
    "    \"\"\"\n",
    "    INPUT\n",
    "    word_list: list of words \n",
    "    OUTPUT\n",
    "    return a list of words that are made up of combinations of the original list\n",
    "    \"\"\"\n",
    "\n",
    "    # Lowercase and joined the list into a single string\n",
    "    lowercase_list = []\n",
    "    for words in word_list:\n",
    "        lowercase_list.append(words.lower())\n",
    "        joined_string = ' '.join(lowercase_list)\n",
    "\n",
    "    # Initialise Tokenizer\n",
    "    tokenizer = WordPunctTokenizer()\n",
    "    tokens = tokenizer.tokenize(joined_string)\n",
    "\n",
    "    bigram_finder = BigramCollocationFinder.from_words(tokens)\n",
    "    bigrams = bigram_finder.nbest(BigramAssocMeasures.chi_sq, 500) # take 500 bigrams with highest chi_sq , pmi\n",
    "\n",
    "    trigram_finder = TrigramCollocationFinder.from_words(tokens)\n",
    "    trigrams = trigram_finder.nbest(TrigramAssocMeasures.chi_sq, 500)\n",
    "\n",
    "    quadgram_finder = QuadgramCollocationFinder.from_words(tokens)\n",
    "    quadgrams = quadgram_finder.nbest(QuadgramAssocMeasures.chi_sq, 500)\n",
    "\n",
    "    # Add biwords to the list of terms\n",
    "    biwords, biperm = [], []\n",
    "    for tuplestring in bigrams:\n",
    "        biwords.append(' '.join(tuplestring))\n",
    "    # Further permutate the words -- swwitching the order of the biwords around\n",
    "    for each_word in biwords:\n",
    "        for each in permutate(each_word):\n",
    "            biperm.append(each)\n",
    "\n",
    "    # Add triwords to the list of terms\n",
    "    triwords, triperm = [], []\n",
    "    for tuplestring in trigrams:\n",
    "        triwords.append(' '.join(tuplestring))\n",
    "    for each_word in triwords: \n",
    "        for each in permutate(each_word):\n",
    "            triperm.append(each)\n",
    "\n",
    "    # Add quadwords to the list of terms\n",
    "    quadwords,qperm = [], []\n",
    "    for tuplestring in quadgrams:\n",
    "        quadwords.append(' '.join(tuplestring))\n",
    "    for each_word in quadwords: \n",
    "        for each in permutate(each_word):\n",
    "            qperm.append(each)  \n",
    "\n",
    "    # return unique set of words\n",
    "    return set(lowercase_list + biwords + biperm + triwords + triperm) #+ quadwords + qperm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54aa673d",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_prob_words = [\"financial advisor\",\n",
    "                   \"financial adviser\",\n",
    "                   \"risk advisor\",\n",
    "                   \"risk adviser\",\n",
    "                   \"medical adviser\",\n",
    "                   \"insurance agent\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec3afd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prohibited terms\n",
    "list_words = synthesize_words(list_prob_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd32395b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "list_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b862da",
   "metadata": {},
   "source": [
    "### Extract for hyperlinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b369d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hyperlinks(data_df, column):\n",
    "    \n",
    "    all_links = []\n",
    "\n",
    "    URL_REGEX = r\"\"\"((?:(?:https|ftp|http)?:(?:/{1,3}|[a-z0-9%])|[a-z0-9.\\-]+[.](?:com|org|uk)/)(?:[^\\s()<>{}\\[\\]]+|\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\))+(?:\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’])|(?:(?<!@)[a-z0-9]+(?:[.\\-][a-z0-9]+)*[.](?:com|uk|ac)\\b/?(?!@)))\"\"\"\n",
    "\n",
    "    #extract hyperlinks\n",
    "    for each_text in range(len(data_df)):\n",
    "        string = data_df.iloc[each_text][column]\n",
    "        all_links.append(re.findall(URL_REGEX, string))\n",
    "\n",
    "    return all_links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc08318",
   "metadata": {},
   "source": [
    "### Extract email links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19adee39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_emails(data_df, column):\n",
    "    \n",
    "    all_emails = []\n",
    "\n",
    "    EMAIL_REGEX = r'[\\w.+-]+@[\\w-]+\\.[\\w.-]+'\n",
    "\n",
    "    # extract emails\n",
    "    for each_text in range(len(data_df)):\n",
    "        string = data_df.iloc[each_text][column]\n",
    "        all_emails.append(re.findall(EMAIL_REGEX, string))\n",
    "        \n",
    "    return all_emails"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06e0b72",
   "metadata": {},
   "source": [
    "### Extract Approval Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a565b69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_codes(data_df, column):\n",
    "    \n",
    "    all_codes = []\n",
    "\n",
    "    APPV_REGEX =r'[a-z][\\d]{5}'\n",
    "\n",
    "    # extract emails\n",
    "    for each_text in range(len(data_df)):\n",
    "        string = data_df.iloc[each_text][column]\n",
    "        all_codes.append(re.findall(EMAIL_REGEX, string))\n",
    "        \n",
    "    return all_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107a35d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df['emails'] = extract_emails(data_df, 'remove_chinese')\n",
    "data_df['hyperlinks'] = extract_hyperlinks(data_df, 'remove_chinese')\n",
    "data_df['approval_code'] = extract_emails(data_df, 'remove_chinese')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be18e87",
   "metadata": {},
   "source": [
    "### Indicator columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6ffdf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df['has_emails']  = np.where(data_df[\"emails\"].str.len() == 0, 0, 1)\n",
    "data_df['has_approvalcode']  = np.where(data_df[\"approval_code\"].str.len() == 0, 0, 1)\n",
    "data_df['has_hyperlinks']  = np.where(data_df[\"hyperlinks\"].str.len() == 0, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32557bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.head(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
