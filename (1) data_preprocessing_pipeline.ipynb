{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Scrapping and Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Facebook Scrapper\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from selenium.webdriver.firefox.firefox_binary import FirefoxBinary\n",
    "\n",
    "# Custom Libraries\n",
    "import src.helpers_scrapper as scrap\n",
    "import src.helpers_preprocess as pp\n",
    "import src.helpers_mlflow as mlf\n",
    "import src.breach_words as breach\n",
    "import src.config as config\n",
    "\n",
    "# import importlib\n",
    "# importlib.reload(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if filepaths exists and create filepaths if do not exist\n",
    "config.create_path(config.output_path)\n",
    "config.create_path(config.raw_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REFRESH_BREACH_LIST = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrap data from Facebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Set configurtions for Mozilla Browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Mozilla Firefox Browser to scrap from facebook\n",
    "options = Options()\n",
    "options.binary = FirefoxBinary(r\"C:\\Program Files\\Mozilla Firefox\\firefox.exe\")\n",
    "options.set_preference(\"browser.download.folderList\",2)\n",
    "options.set_preference(\"browser.download.manager.showWhenStarting\", False)\n",
    "options.set_preference(\"browser.download.dir\",\"/Data\")\n",
    "options.set_preference(\"browser.helperApps.neverAsk.saveToDisk\", \"application/octet-stream,application/vnd.ms-excel\")\n",
    "driver = webdriver.Firefox(executable_path=r\"C:\\Users\\xtanl\\miniconda3\\Lib\\site-packages\\selenium\\webdriver\\geckodriver.exe\", options=options)\n",
    "\n",
    "#driver.get('http://google.com/') # brings up the browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the Facebook_scraper class\n",
    "posts_count = 100\n",
    "browser = \"firefox\"\n",
    "timeout = 60  #seconds\n",
    "headless = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set facebook users to scrap\n",
    "df1 = scrap.extract_json(\"NicholasGohOrganisation\", posts_count, browser, timeout, headless)\n",
    "df2 = scrap.extract_json(\"QualityLifeWithQiLe\", posts_count, browser, timeout, headless)\n",
    "df3 = scrap.extract_json(\"AinWallofTrust\", posts_count, browser, timeout, headless)\n",
    "df4 = scrap.extract_json(\"JayceeOngFC\", posts_count, browser, timeout, headless)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine user datasets\n",
    "fb_df = pd.concat([df1, df2, df3, df4])\n",
    "print(f\"fb_df output: {fb_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the dataset\n",
    "config.export_file_csv(fb_df, config.fb_media_datapath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrap data from Instagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialise a instagraphi client \n",
    "insta_client = scrap.login_instgram( config.insta_username, config.insta_pwd )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set list of users to extract from\n",
    "instagram_users = [\n",
    "\n",
    "        'phiyphiy'\n",
    "        ,'angelicaasimm'\n",
    "        ,'augustineseah'\n",
    "        ,'abrialpang'\n",
    "        ,'yul_dewi'\n",
    "        ,'jocelynkau'\n",
    "        ,'headlights_'\n",
    "        ,'jasperseahassociates'\n",
    "        ,'yugitoh'\n",
    "        ,'ato.par'\n",
    "        ,'agent_e'\n",
    "        ,'teamey_'\n",
    "        ,'danny_c'\n",
    "        ,'ryankoh'\n",
    "        ,'alanyey'\n",
    "        ,'geralds'\n",
    "        ,'33lespe'\n",
    "        ,'xie.xie'\n",
    "        ,'sandrao'\n",
    "]\n",
    "\n",
    "insta_user_df, insta_df = scrap.instagram_scrapper(insta_client, instagram_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export scrapped instagram post to specified filepath\n",
    "config.export_file_csv(insta_user_df, config.insta_user_datapath)\n",
    "config.export_file_csv(insta_df, config.insta_media_datapath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Session\n",
    "# scrap.check_session(insta_client, config.insta_username, config.insta_pwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pp.combine_datasets(fb_df, insta_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean the content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df['cleaned_text'] = pp.clean_text(data_df, 'content')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Initialise the potential breach list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not REFRESH_BREACH_LIST: \n",
    "    all_breachlist_files = [os.path.join(config.raw_data_path, x) for x in os.listdir(config.raw_data_path) if x.startswith(\"breach_list\") and x.endswith(\".pkl\")]\n",
    "    if all_breachlist_files == []:\n",
    "        print(\"Breach Wordlist does not exists. Set REFRESH_BREACH_LIST = True.\")\n",
    "    else:\n",
    "        # Read breach wordlist from filepath\n",
    "        curr_breachlist_filepath = max(all_breachlist_files, key = os.path.getctime)\n",
    "        with open (curr_breachlist_filepath, 'rb') as fp:\n",
    "            text_breach = pickle.load(fp)\n",
    "        print(f\"Potential Breach Words Loaded from {config.breachlist_datapath}\")\n",
    "else:\n",
    "    print(\"Synthesizing new list of potential breach words...\")\n",
    "    # Synthesize potential breach words from a list of words specified in src/breach_words.py\n",
    "    potential_breach_desig = list(breach.synthesize_words(breach.given_list_of_designations))\n",
    "    potential_breach_hashtags = [x.lower().replace(' ', '_') for x in potential_breach_desig]\n",
    "    potential_breach_promos = list(breach.synthesize_words(breach.given_list_of_promo))\n",
    "    text_breach = [x.lower() for x in potential_breach_desig + potential_breach_promos]\n",
    "    # Replace the breach wordlist\n",
    "    with open(config.breachlist_datapath, 'wb') as fp:\n",
    "        pickle.dump(text_breach, fp)\n",
    "    print(f\"Potential Breach Words Updated into {config.breachlist_datapath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Download spacy english package\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select which features to create\n",
    "output_features = ['data_source', 'id', 'username', 'posted_on', 'content', 'cleaned_text', #'hashtags', 'mentions', 'emojis', \n",
    "                    'breach_flagwords', 'breach_hashes', 'has_nonpru_email', 'has_hyperlinks', 'has_disclaimer']\n",
    "\n",
    "data_df = pp.create_features(data_df, 'content', output_features, text_breach, potential_breach_hashtags)\n",
    "\n",
    "# Add NER features - contains_monetary\n",
    "data_df = pp.get_ner_features(data_df)\n",
    "\n",
    "# Replace null content with a string 'None'\n",
    "data_df.loc[data_df['cleaned_text'].isna(), 'cleaned_text'] = 'None'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.export_file_csv(data_df, config.feature_data, mode='w+')\n",
    "print(f\"Dataset with {list(data_df.columns)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
