{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Pycaret\n",
    "#from pycaret.regression import setup\n",
    "# import pycaret.nlp as pycnlp\n",
    "# import pycaret.classification as pyclass\n",
    "# from pycaret.classification import get_config, predict_model, plot_model, pull\n",
    "\n",
    "import pycaret\n",
    "from pycaret.classification import *\n",
    "\n",
    "# MLFlow\n",
    "import mlflow\n",
    "import mlflow.keras\n",
    "from mlflow.tracking import MlflowClient\n",
    "from mlflow.utils.mlflow_tags import MLFLOW_PARENT_RUN_ID, MLFLOW_RUN_NAME\n",
    "\n",
    "# NLP\n",
    "import spacy\n",
    "import category_encoders as ce\n",
    "import pickle\n",
    "\n",
    "# LSTM\n",
    "import keras\n",
    "from keras.preprocessing.text import one_hot, Tokenizer\n",
    "# from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.utils import pad_sequences\n",
    "\n",
    "\n",
    "# For Coherence Score\n",
    "import gensim\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel, LdaMulticore, CoherenceModel, LsiModel, HdpModel\n",
    "# For Text Vectorization\n",
    "from gensim.models import word2vec\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rcParams['font.family'] = ['DejaVu Sans']\n",
    "\n",
    "import src.helpers_preprocess as pp\n",
    "import src.breach_words as breach\n",
    "import src.helpers_mlflow as mlf\n",
    "import src.helpers_evaluation as ev\n",
    "import src.config as config\n",
    "\n",
    "import importlib\n",
    "importlib.reload(mlf)\n",
    "\n",
    "#pd.set_option('max_colwidth', -1)a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_class = 'incompliant'\n",
    "\n",
    "RESET_WORKINGDIR = True\n",
    "SAVE_PYCARET_DATA = True\n",
    "SAVE_DECILES = True\n",
    "SAVE_PROB_DIST = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the working directory\n",
    "if RESET_WORKINGDIR | len(os.listdir(config.output_path)) == 0:\n",
    "    print(\"Not resetting the working directory.\")\n",
    "else:\n",
    "    # Reset working directory\n",
    "    for filename in os.listdir(config.output_path):\n",
    "        file_path = os.path.join(config.output_path, filename)\n",
    "        try:\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)\n",
    "            elif os.path.isdir(file_path):\n",
    "                shutil.rmtree(file_path)\n",
    "        except Exception as e:\n",
    "            print('Failed to delete %s. Reason: %s' % (file_path, e))\n",
    "            \n",
    "    print(\"Reset the working directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if filepaths exists and create filepaths if do not exist\n",
    "# Explainations for filepaths can be found in config.py\n",
    "\n",
    "config.create_path(config.main_directory)\n",
    "config.create_path(config.data_artifact_path)\n",
    "config.create_path(config.exai_artifact_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import features dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get latest feature set\n",
    "data_df = config.get_latest_csv(config.raw_data_path, 'full_features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLFLOW Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup the MYSQL database for tracking MLFLOW Model Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create database if it does not exists\n",
    "# Database required for MLFlow model registry as only certain APIs are supported by Mlflow\n",
    "mlf.create_database_storage(config.dbServerName, config.dbPort, config.dbName, config.dbUser, config.dbPassword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if database exists\n",
    "# mlf.show_databases(dbServerName, dbUser, dbPassword)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup MLFLOW to track experiments and model registry in MYSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mlflow command run configured in src/config.py\n",
    "mlflow_conn = mlf.create_mlflow_cmd(config.storage_filepath, config.dbName, config.dbUser, config.dbPassword, config.dbServerName, config.dbPort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ⚠️ Run this command in anaconda environment, if running thru Jupyter, interrupt this command line after ahwile.\n",
    "!mlflow ui \\\n",
    "--backend-store-uri file:/./aicritic_mlflow \\\n",
    "--registry-store-uri mysql+pymysql://root:<dbName>@localhost:<port>/mlflow_tracking_database \\\n",
    "--host 127.0.0.1 --port 5000 \\\n",
    "--serve-artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_id, client = mlf.setup_mlflow(config.exp_name, config.storage_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init MLFlow start run\n",
    "parent_run = client.create_run(experiment_id=exp_id, run_name = f'classification_{target_class}')\n",
    "parent_run_id = parent_run.info.run_uuid\n",
    "client.log_param(parent_run_id, \"run_id\", parent_run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set columns not be used during ML modelling, these features are avoided as XGboost was used.\n",
    "ignored_cols = ['data_source', 'username', 'posted_on', 'content', 'cleaned_text', 'hashtags', 'mentions', 'emojis', 'ner']\n",
    "# Set columns to be one hot encoded\n",
    "categorical_cols = ['breach_flagwords', 'breach_hashes', 'has_nonpru_email', 'has_hyperlinks', 'has_disclaimer', 'contains_monetary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode specified features\n",
    "data_encoded, enc_cols = pp.get_onehot(data_df.set_index('id'),\n",
    "                                        feature_list = categorical_cols,\n",
    "                                        save_dir = config.encoder_artifact_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test Split\n",
    "Doing the train test split out of Pycaret as the ML model (XGBoost) needs to be aligned with the NN Model (LSTM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split( data_encoded.drop(columns=['incompliant']), data_encoded['incompliant'], test_size=0.33, random_state=42 )\n",
    "\n",
    "X_train['incompliant'] = y_train\n",
    "X_valid['incompliant'] = y_valid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rule Based Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create child run for ML Classification\n",
    "ml_run = client.create_run(\n",
    "        experiment_id=exp_id,\n",
    "        run_name = f'ml_model_{target_class}',\n",
    "        tags={\n",
    "            MLFLOW_PARENT_RUN_ID : parent_run_id\n",
    "            }\n",
    "        )\n",
    "\n",
    "print(\"ml_run_id\")\n",
    "ml_run_id = ml_run.info.run_uuid\n",
    "client.log_param(ml_run_id, \"run_id\", ml_run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Classification\n",
    "classfication_exp = setup( data= X_train,\n",
    "                        target = 'incompliant',\n",
    "                        test_data = X_valid,\n",
    "                        ignore_features = ignored_cols,\n",
    "                        # categorical_features = enc_cols,\n",
    "                        preprocess=False,\n",
    "                        session_id=42,\n",
    "                        n_jobs=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best_model\n",
    "classfication_exp.compare_models(include = ['lr','ridge', 'lda', 'rf', 'knn','nb','svm', 'gbc', 'ada', 'et', 'qda', 'dt', 'xgboost' ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dataset used for testing models\n",
    "training_df = get_config('X_train')\n",
    "training_df['incompliant'] = get_config('y_train')\n",
    "validation_df = get_config('X_test')\n",
    "validation_df['incompliant'] = get_config('y_test')\n",
    "\n",
    "# Save the dataset for replicating training\n",
    "if SAVE_PYCARET_DATA:\n",
    "    training_df.to_csv(config.data_artifact_path  + f\"\\\\{config.today}_training_{parent_run_id[:5]}.csv\")\n",
    "    validation_df.to_csv(config.data_artifact_path  + f\"\\\\{config.today}_validation_{parent_run_id[:5]}.csv\")\n",
    "\n",
    "    print(f\"Training saved as: {config.data_artifact_path }\" + f\"\\\\{config.today}_training_{parent_run_id[:5]}.csv\")\n",
    "    print(f\"Validation saved as: {config.data_artifact_path }\" + f\"\\\\{config.today}_validation_{parent_run_id[:5]}.csv\")\n",
    "else:\n",
    "    print(\"Train/Test Split not saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choice Model - XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "xgb_instance = xgb.XGBClassifier()\n",
    "fit_kwargs = { \"early_stopping_rounds\": 5, \"eval_metric\": \"logloss\", \"eval_set\": [(get_config('X_test'), get_config('y_test'))]}\n",
    "xgb_model = classfication_exp.create_model(xgb_instance, fit_kwargs=fit_kwargs, error_score ='raise')\n",
    "\n",
    "tuned_model = xgb_model\n",
    "print(f'Classifier used: {tuned_model.__class__.__name__}')\n",
    "client.log_param(ml_run_id, \"model_type\", f\"{tuned_model.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pycaret output predictions\n",
    "predictions = predict_model(tuned_model, data=validation_df, raw_score=True)\n",
    "\n",
    "# Prepare to log result metrics into MLflow\n",
    "ml_auc = roc_auc_score(y_full_data, rb_test_pred['prediction_score_1'])\n",
    "ml_acc = accuracy_score(y_full_data, np.array(rb_test_pred['prediction_label']))\n",
    "ml_prec = precision_score(y_full_data, np.array(rb_test_pred['prediction_label']), average='binary')\n",
    "ml_recall = recall_score(y_full_data, np.array(rb_test_pred['prediction_label']), average='binary')\n",
    "ml_f1 = f1_score(y_full_data, np.array(rb_test_pred['prediction_label']), average='binary')\n",
    "\n",
    "print('ml_auc', ml_auc, '\\nml_acc', ml_acc, '\\nml_prec', ml_prec, '\\nml_recall', ml_recall, '\\nml_f1', ml_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save ML model\n",
    "ml_signature = mlflow.models.infer_signature( model_input = pd.DataFrame(X_train), \n",
    "                                              model_output = pd.DataFrame(predictions['prediction_label']))\n",
    "\n",
    "mlflow.sklearn.save_model(tuned_model, \n",
    "                            config.ml_artifact_path,\n",
    "                            signature = ml_signature )\n",
    "\n",
    "print(f'Model has been saved at: {config.ml_artifact_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on validation (out of time dataset)\n",
    "predictions['Score_1_round'] = round(predictions['prediction_score_1'], 1)\n",
    "decile_table = ev.get_decile_score(predictions, f'{target_class}', 'prediction_label', 'Score_1_round')\n",
    "decile_table\n",
    "if SAVE_DECILES:\n",
    "    # Save decile table\n",
    "    decile_table.to_csv(os.path.join(config.exai_artifact_path, f\"decile_table_{config.today}.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show probability distribution box plot\n",
    "pred_correct = predictions[predictions[f'{target_class}'] == predictions['prediction_label']]\n",
    "prob_dist = pd.DataFrame(pd.Series([round(x*100) for x in pred_correct.Score_1_round]).value_counts()).reset_index().rename(columns={0: 'count', 'index': 'probability of incompliancy (%)'})\n",
    "prob_dist\n",
    "\n",
    "if SAVE_PROB_DIST:\n",
    "    # Save probabliity distribution table\n",
    "    prob_dist.to_csv(os.path.join(config.exai_artifact_path, f\"prob_dist_{config.today}.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(xgb_model, plot = 'confusion_matrix', plot_kwargs = {'percent' : False})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Log ML Model artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log all artifacts\n",
    "with mlflow.start_run(run_id=ml_run_id):\n",
    "    # mlflow.log_metrics(metrics = results_dict)\n",
    "    mlflow.log_metrics(metrics={\"AUC\": ml_auc})\n",
    "    mlflow.log_metrics(metrics={\"Accuracy\": ml_acc})\n",
    "    mlflow.log_metrics(metrics={\"Prec.\": ml_prec})\n",
    "    mlflow.log_metrics(metrics={\"Recall\": ml_recall})\n",
    "    mlflow.log_metrics(metrics={\"F1\": ml_f1})\n",
    "    \n",
    "    mlflow.log_artifacts(filepaths_dict['exai_artifact_path'], \"Results\")\n",
    "    mlflow.log_artifacts(filepaths_dict['ml_artifact_path'], \"model\")\n",
    "\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# End ML run and get status\n",
    "client.set_terminated(ml_run.info.run_id, status=\"FINISHED\")\n",
    "ml_run = client.get_run(ml_run.info.run_id)\n",
    "print(f\"run_id: {ml_run.info.run_id}; status: {ml_run.info.status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise LSTM run\n",
    "lstm_run = client.create_run(\n",
    "        experiment_id=exp_id,\n",
    "        run_name = f'lstm_model_{target_class}',\n",
    "        tags={\n",
    "            MLFLOW_PARENT_RUN_ID : parent_run_id\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"lstm_run_id\")\n",
    "lstm_run_id = lstm_run.info.run_uuid\n",
    "client.log_param(lstm_run_id, \"run_id\", lstm_run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing Data from LSTM Model\n",
    "y_train = np.array(X_train[f'{target_class}'])\n",
    "y_valid = np.array(X_valid[f'{target_class}'])\n",
    "\n",
    "X_train = X_train['cleaned_text'].astype(\"str\")\n",
    "X_valid = X_valid['cleaned_text'].astype(\"str\")\n",
    "\n",
    "print('train_set', len(X_train), 'validation_set', len(X_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokenizer = Tokenizer()\n",
    "word_tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# Convert Text to Sequence\n",
    "X_train = word_tokenizer.texts_to_sequences(X_train)\n",
    "X_valid = word_tokenizer.texts_to_sequences(X_valid)\n",
    "\n",
    "# Padding all reviews to fixed length 100\n",
    "maxlen = 100\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "X_valid = pad_sequences(X_valid, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tokenizer\n",
    "if not os.path.exists(config.tokenizer_artifact_path):\n",
    "    os.makedirs(config.tokenizer_artifact_path)\n",
    "with open(config.tokenizer_artifact_path + '\\\\tokenizer.pkl', 'wb') as outfile:\n",
    "    pickle.dump(word_tokenizer, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import asarray\n",
    "\n",
    "# Load GloVe word embeddings and create an Embeddings Dictionary\n",
    "embeddings_dictionary = {}\n",
    "glove_file = open('inputs\\\\glove.6B.100d.txt', encoding=\"utf8\")\n",
    "\n",
    "for line in glove_file:\n",
    "    records = line.split()\n",
    "    word = records[0]\n",
    "    vector_dimensions = asarray(records[1:], dtype='float32')\n",
    "    embeddings_dictionary [word] = vector_dimensions\n",
    "\n",
    "glove_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding 1 to store dimensions for words for which no pretrained word embeddings exist.\n",
    "vocab_length = len(word_tokenizer.word_index) + 1\n",
    "print(\"vocab_length: \", vocab_length)\n",
    "\n",
    "# Create Embedding matrix\n",
    "# Containing 100-dimensional GloVe word embeddings for all words in our corpus.\n",
    "embedding_matrix = np.zeros((vocab_length, 100))\n",
    "for word, index in word_tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_dictionary.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the LSTM model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, GlobalMaxPooling1D, Embedding, Conv1D, LSTM, Bidirectional, Activation, Dropout, Dense\n",
    "\n",
    "lstm_model = Sequential()\n",
    "embedding_layer = Embedding(vocab_length, 100, weights=[embedding_matrix], input_length=maxlen , trainable=True)\n",
    "lstm_model.add(embedding_layer)\n",
    "lstm_model.add(Dropout(0.5))\n",
    "lstm_model.add(Bidirectional(LSTM(128)))\n",
    "lstm_model.add(Dense(1, activation='sigmoid')) # Binary\n",
    "\n",
    "# Display Model\n",
    "lstm_model.summary()\n",
    "\n",
    "# Model compiling\n",
    "lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Model Training\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "lstm_model_history = lstm_model.fit(X_train, y_train, batch_size=128, epochs=100, verbose=1, validation_split=0.3, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev.plot_lstm_performance(lstm_model_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions in validation dataset for combining with ml models\n",
    "y_pred_valid = lstm_model.predict(X_train)\n",
    "y_pred_test = lstm_model.predict(X_valid)\n",
    "\n",
    "lstm_valid_pred = pd.DataFrame(y_pred_valid, columns=['lstm_pred_score'])\n",
    "lstm_valid_pred['lstm_pred'] = np.where(lstm_valid_pred['lstm_pred_score'] < 0.50, 0, 1)\n",
    "lstm_valid_pred = lstm_valid_pred.merge(pd.DataFrame(validation_df.index).reset_index(drop=True), how='left', left_index=True, right_index=True)\n",
    "\n",
    "lstm_test_pred = pd.DataFrame(y_pred_test, columns=['lstm_pred_score'])\n",
    "lstm_test_pred['lstm_pred'] = np.where(lstm_test_pred['lstm_pred_score'] < 0.50, 0, 1)\n",
    "lstm_test_pred = lstm_test_pred.merge(pd.DataFrame(validation_df.index).reset_index(drop=True), how='left', left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_auc = roc_auc_score(y_valid, lstm_valid_pred['lstm_pred_score'])\n",
    "dl_acc = accuracy_score(y_valid, np.array(lstm_valid_pred['lstm_pred']))\n",
    "dl_prec = precision_score(y_valid, np.array(lstm_valid_pred['lstm_pred']), average='binary')\n",
    "dl_recall = recall_score(y_valid, np.array(lstm_valid_pred['lstm_pred']), average='binary')\n",
    "dl_f1 = f1_score(y_valid, np.array(lstm_valid_pred['lstm_pred']), average='binary')\n",
    "\n",
    "print('dl_auc', dl_auc, '\\ndl_acc', dl_acc, '\\ndl_prec', dl_prec, '\\ndl_recall', dl_recall, '\\ndl_f1', dl_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "dl_signature = mlflow.models.infer_signature( model_input = X_valid,\n",
    "                                            model_output = y_pred_test )\n",
    "\n",
    "mlflow.keras.save_model(lstm_model,\n",
    "                        config.nn_artifact_path,\n",
    "                        signature = dl_signature )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log all artifacts\n",
    "with mlflow.start_run(run_id=lstm_run_id):\n",
    "    mlflow.log_metrics(metrics={\"AUC\": dl_auc})\n",
    "    mlflow.log_metrics(metrics={\"Accuracy\": dl_acc})\n",
    "    mlflow.log_metrics(metrics={\"Prec.\": dl_prec})\n",
    "    mlflow.log_metrics(metrics={\"Recall\": dl_recall})\n",
    "    mlflow.log_metrics(metrics={\"F1\": dl_f1})\n",
    "\n",
    "    mlflow.log_artifacts(config.exai_artifact_path, \"Results\")\n",
    "    mlflow.log_artifacts(config.tokenizer_artifact_path, \"Tokenizer\")\n",
    "    mlflow.log_artifacts(config.nn_artifact_path, \"model\")\n",
    "\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# End LSTM run and get status\n",
    "client.set_terminated(lstm_run.info.run_id, status=\"FINISHED\")\n",
    "lstm_run = client.get_run(lstm_run.info.run_id)\n",
    "print(f\"run_id: {lstm_run.info.run_id}; status: {lstm_run.info.status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined prediction labels - validation data\n",
    "rule_based_prediction = predictions[[f'{target_class}', 'prediction_label', 'prediction_score_1']].rename({'prediction_label': 'rb_pred', 'prediction_score_1':'rb_pred_score'}, axis=1)\n",
    "combined_pred = lstm_valid_pred.merge(rule_based_prediction, how='left', left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine split between models\n",
    "correct = []\n",
    "\n",
    "for dec_increment in range(40, 101):\n",
    "  lstm_wt = dec_increment/ 100\n",
    "  combined_pred['combined_score'] = (lstm_wt*combined_pred['lstm_pred_score'] + (1-lstm_wt)*combined_pred['rb_pred_score'])\n",
    "  combined_pred['combined_pred'] = np.where(combined_pred['combined_score'] < 0.50, 0, 1)\n",
    "  right_prop = (combined_pred['combined_pred'] == combined_pred[f'{target_class}']).value_counts().iloc[0] / len(combined_pred)\n",
    "  correct.append((lstm_wt,right_prop))\n",
    "\n",
    "correct = pd.DataFrame(correct).rename({0: 'weight', 1:'accuracy'}, axis=1)\n",
    "plt.plot(correct['weight'], correct['accuracy'])\n",
    "plt.ylabel(\"Combined Accuracy (%)\")\n",
    "plt.xlabel(\"Proportion of LSTM prediction scores used\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the weights that produce the best score\n",
    "lstm_wt = correct[correct.accuracy == correct.accuracy.max()]['weight'].min()\n",
    "\n",
    "# Validate on test data (2023)\n",
    "combined_pred['combined_score'] = (lstm_wt*combined_pred['lstm_pred_score'] + (1-lstm_wt)*combined_pred['rb_pred_score'])\n",
    "combined_pred['combined_pred'] = np.where(combined_pred['combined_score'] < 0.5, 0, 1)\n",
    "\n",
    "final_acc = (combined_pred['combined_pred'] == combined_pred[f'{target_class}']).value_counts().iloc[0] / len(combined_pred)\n",
    "# Rearrange columns\n",
    "combined_test = combined_pred[['id', f'{target_class}', 'combined_score', 'combined_pred', 'lstm_pred_score', 'lstm_pred',\n",
    "                               'rb_pred_score', 'rb_pred']]\n",
    "combined_test.to_csv( os.path.join(config.exai_artifact_path + f\"\\\\prediction_{config.today}.csv\") )\n",
    "\n",
    "print(f\"Combined Accuracy for [ML wt. {1-lstm_wt} | LSTM wt. {lstm_wt}]:\", round(final_acc, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_auc = roc_auc_score(y_valid, combined_test['combined_score'])\n",
    "final_acc = accuracy_score(y_valid, np.array(combined_test['combined_pred']))\n",
    "final_prec = precision_score(y_valid, np.array(combined_test['combined_pred']), average='binary')\n",
    "final_recall = recall_score(y_valid, np.array(combined_test['combined_pred']), average='binary')\n",
    "final_f1 = f1_score(y_valid, np.array(combined_test['combined_pred']), average='binary')\n",
    "\n",
    "print('final_auc', final_auc, '\\nfinal_acc', final_acc, '\\nfinal_prec', final_prec, '\\nfinal_recall', final_recall, '\\nfinal_f1', final_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Log artifacts and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log final metrics\n",
    "with mlflow.start_run(run_id=parent_run_id):\n",
    "    mlflow.log_param(\"model_wt\", lstm_wt)\n",
    "    mlflow.log_metrics(metrics={\"AUC\": final_auc})\n",
    "    mlflow.log_metrics(metrics={\"Accuracy\": final_acc})\n",
    "    mlflow.log_metrics(metrics={\"Prec.\": final_prec})\n",
    "    mlflow.log_metrics(metrics={\"Recall\": final_recall})\n",
    "    mlflow.log_metrics(metrics={\"F1\": final_f1})\n",
    "    \n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# End parent run and get status of Mlflow runs\n",
    "client.set_terminated(parent_run.info.run_id, status=\"FINISHED\")\n",
    "parent_run = client.get_run(parent_run.info.run_id)\n",
    "\n",
    "print(f\"run_id: {parent_run.info.run_id}; status: {parent_run.info.status}\")\n",
    "print(f\"run_id: {ml_run.info.run_id}; status: {ml_run.info.status}\")\n",
    "print(f\"run_id: {lstm_run.info.run_id}; status: {lstm_run.info.status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Register Model into MLFlow model registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if all basic metrics are above/Below the threshold\n",
    "if ((final_acc > 0.6)):\n",
    "  is_model_good = 'yes'\n",
    "else:\n",
    "  is_model_good = 'no'\n",
    "\n",
    "print(\"is_model_good: \", is_model_good, \", final_accuracy is\", final_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check existing model and compare, Based on outcome register this new model\n",
    "if is_model_good == 'yes':\n",
    "    mlf.mlflow_existing_model_compare_and_registry(client= client,\n",
    "                                                   model_run_info=ml_run, \n",
    "                                                   registry_model_name=f'XGBoost_{target_class}',\n",
    "                                                   pos_metrics_list=['Accuracy'],\n",
    "                                                   pos_metrics_thresh_diff_list=[0.01],\n",
    "                                                   model_type=\"Classification\")\n",
    "    \n",
    "    mlf.mlflow_existing_model_compare_and_registry(client= client,\n",
    "                                                   model_run_info=lstm_run, \n",
    "                                                   registry_model_name=f'LSTM_{target_class}',\n",
    "                                                   pos_metrics_list=['Accuracy'],\n",
    "                                                   pos_metrics_thresh_diff_list=[0.01],\n",
    "                                                   model_type=\"Classification\")\n",
    "else:\n",
    "    None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of Script"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
